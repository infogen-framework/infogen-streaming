[framework] 2015-12-14 17:05:24,312 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -11453 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:05:24,328 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -11469 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.fs.FileAlreadyExistsException: /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1628)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:48)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:121)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	... 15 more
[framework] 2015-12-14 17:05:25,397 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -12538 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:05:25,428 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -12569 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.fs.FileAlreadyExistsException: /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1628)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:48)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:121)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	... 15 more
[framework] 2015-12-14 17:05:26,517 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -13658 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:05:26,544 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -13685 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.fs.FileAlreadyExistsException: /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1628)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:48)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:121)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	... 15 more
[framework] 2015-12-14 17:05:26,604 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -13745 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:05:26,620 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -13761 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.fs.FileAlreadyExistsException: /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1628)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:48)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:121)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /infogen/output/infogen_topic_tracking/1/4494174- for client 192.168.100.203 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2738)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	... 15 more
[framework] 2015-12-14 17:06:45,946 - com.infogen.zookeeper.InfoGen_ZooKeeper -0    [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:06:45,994 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[framework] 2015-12-14 17:06:45,994 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:host.name=localhost
[framework] 2015-12-14 17:06:45,995 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.version=1.8.0_20
[framework] 2015-12-14 17:06:45,995 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.vendor=Oracle Corporation
[framework] 2015-12-14 17:06:45,995 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.home=/usr/lib/jvm/java-8-sun/jre
[framework] 2015-12-14 17:06:45,995 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.class.path=/home/juxinli/workspace/infogen_etl/target/classes:/home/juxinli/workspace/infogen_etl/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-client/2.7.1/hadoop-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-common/2.7.1/hadoop-common-2.7.1.jar:/home/juxinli/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/home/juxinli/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/juxinli/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/juxinli/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/juxinli/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/home/juxinli/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/juxinli/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/juxinli/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/juxinli/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/juxinli/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/juxinli/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/juxinli/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/juxinli/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/juxinli/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/juxinli/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/juxinli/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/juxinli/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-auth/2.7.1/hadoop-auth-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/home/juxinli/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/juxinli/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/juxinli/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.7.1/hadoop-hdfs-2.7.1.jar:/home/juxinli/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/juxinli/.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/home/juxinli/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/juxinli/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/juxinli/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.1/hadoop-mapreduce-client-app-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.1/hadoop-mapreduce-client-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.7.1/hadoop-yarn-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.7.1/hadoop-yarn-server-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.1/hadoop-mapreduce-client-shuffle-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.7.1/hadoop-yarn-api-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.1/hadoop-mapreduce-client-core-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.7.1/hadoop-yarn-common-2.7.1.jar:/home/juxinli/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/juxinli/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/juxinli/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/juxinli/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.1/hadoop-mapreduce-client-jobclient-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.1/hadoop-annotations-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka_2.11/0.8.2.2/kafka_2.11-0.8.2.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/juxinli/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/home/juxinli/.m2/repository/org/scala-lang/scala-library/2.11.5/scala-library-2.11.5.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka-clients/0.8.2.2/kafka-clients-0.8.2.2.jar:/home/juxinli/.m2/repository/org/xerial/snappy/snappy-java/1.1.1.7/snappy-java-1.1.1.7.jar:/home/juxinli/.m2/repository/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar:/home/juxinli/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/juxinli/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/juxinli/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/juxinli/.m2/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar
[framework] 2015-12-14 17:06:45,996 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[framework] 2015-12-14 17:06:45,996 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.io.tmpdir=/tmp
[framework] 2015-12-14 17:06:45,996 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.compiler=<NA>
[framework] 2015-12-14 17:06:45,996 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.name=Linux
[framework] 2015-12-14 17:06:45,996 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.arch=amd64
[framework] 2015-12-14 17:06:45,996 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.version=4.2.0-19-generic
[framework] 2015-12-14 17:06:45,997 - org.apache.zookeeper.ZooKeeper -51   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.name=juxinli
[framework] 2015-12-14 17:06:45,997 - org.apache.zookeeper.ZooKeeper -51   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.home=/home/juxinli
[framework] 2015-12-14 17:06:45,997 - org.apache.zookeeper.ZooKeeper -51   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.dir=/home/juxinli/workspace/infogen_etl
[framework] 2015-12-14 17:06:45,998 - org.apache.zookeeper.ZooKeeper -52   [main] INFO  org.apache.zookeeper.ZooKeeper  - Initiating client connection, connectString=172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181 sessionTimeout=10000 watcher=com.infogen.zookeeper.InfoGen_ZooKeeper$1@73846619
[framework] 2015-12-14 17:06:46,029 - com.infogen.zookeeper.InfoGen_ZooKeeper -83   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper成功:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:06:46,029 - com.infogen.zookeeper.InfoGen_ZooKeeper -83   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录:/brokers/ids
[framework] 2015-12-14 17:06:46,037 - org.apache.zookeeper.ClientCnxn -91   [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 172.16.8.97/172.16.8.97:2181. Will not attempt to authenticate using SASL (unknown error)
[framework] 2015-12-14 17:06:46,120 - org.apache.zookeeper.ClientCnxn -174  [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Socket connection established to 172.16.8.97/172.16.8.97:2181, initiating session
[framework] 2015-12-14 17:06:46,141 - org.apache.zookeeper.ClientCnxn -195  [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 172.16.8.97/172.16.8.97:2181, sessionid = 0x614f1aad4d9c8033, negotiated timeout = 10000
[framework] 2015-12-14 17:06:46,144 - com.infogen.zookeeper.InfoGen_ZooKeeper -198  [main-EventThread] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 连接事件  path:null  state:SyncConnected  type:None
[framework] 2015-12-14 17:06:46,157 - com.infogen.zookeeper.InfoGen_ZooKeeper -211  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录成功:/brokers/ids
[framework] 2015-12-14 17:06:46,158 - com.infogen.zookeeper.InfoGen_ZooKeeper -212  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/1
[framework] 2015-12-14 17:06:46,176 - com.infogen.zookeeper.InfoGen_ZooKeeper -230  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/1
[framework] 2015-12-14 17:06:46,176 - com.infogen.zookeeper.InfoGen_ZooKeeper -230  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/2
[framework] 2015-12-14 17:06:46,190 - com.infogen.zookeeper.InfoGen_ZooKeeper -244  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/2
[framework] 2015-12-14 17:06:46,190 - com.infogen.zookeeper.InfoGen_ZooKeeper -244  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/3
[framework] 2015-12-14 17:06:46,204 - com.infogen.zookeeper.InfoGen_ZooKeeper -258  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/3
[framework] 2015-12-14 17:06:46,204 - com.infogen.zookeeper.InfoGen_ZooKeeper -258  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:06:46,230 - com.infogen.zookeeper.InfoGen_ZooKeeper -284  [main] ERROR com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据错误: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /infogen_consumers/infogen_topic_tracking/1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1184)
	at com.infogen.zookeeper.InfoGen_ZooKeeper.get_data(InfoGen_ZooKeeper.java:155)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:110)
[framework] 2015-12-14 17:06:46,234 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -288  [main] INFO  com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #使用默认 offset为:0
[framework] 2015-12-14 17:06:46,925 - com.infogen.kafka.InfoGen_Consumer -979  [main] WARN  com.infogen.kafka.InfoGen_Consumer  - #offset不存在-从最早的offset开始获取:4494174
[framework] 2015-12-14 17:06:47,422 - org.apache.hadoop.util.NativeCodeLoader -1476 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[framework] 2015-12-14 17:06:48,413 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -2467 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:06:48,593 - com.hadoop.compression.lzo.GPLNativeCodeLoader -2647 [main] INFO  com.hadoop.compression.lzo.GPLNativeCodeLoader  - Loaded native gpl library from the embedded binaries
[framework] 2015-12-14 17:06:48,599 - com.hadoop.compression.lzo.LzoCodec -2653 [main] INFO  com.hadoop.compression.lzo.LzoCodec  - Successfully loaded & initialized native-lzo library [hadoop-lzo rev 826e7d8d3e839964dd9ed2d5f83296254b2c71d3]
[framework] 2015-12-14 17:06:48,602 - org.apache.hadoop.conf.Configuration.deprecation -2656 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[framework] 2015-12-14 17:06:48,603 - org.apache.hadoop.io.compress.CodecPool -2657 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 17:07:47,055 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -61109 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:07:47,539 - com.infogen.zookeeper.InfoGen_ZooKeeper -61593 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:07:47,558 - com.infogen.zookeeper.InfoGen_ZooKeeper -61612 [Thread-0] ERROR com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据失败: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /infogen_consumers/infogen_topic_tracking/1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)
	at com.infogen.zookeeper.InfoGen_ZooKeeper.set_data(InfoGen_ZooKeeper.java:169)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.lambda$0(Kafka_To_Hdfs.java:80)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs$$Lambda$2/2092769598.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:745)
[framework] 2015-12-14 17:07:50,975 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -65029 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675049-
[framework] 2015-12-14 17:11:45,532 - com.infogen.zookeeper.InfoGen_ZooKeeper -0    [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:11:45,574 - org.apache.zookeeper.ZooKeeper -42   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[framework] 2015-12-14 17:11:45,574 - org.apache.zookeeper.ZooKeeper -42   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:host.name=localhost
[framework] 2015-12-14 17:11:45,574 - org.apache.zookeeper.ZooKeeper -42   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.version=1.8.0_20
[framework] 2015-12-14 17:11:45,574 - org.apache.zookeeper.ZooKeeper -42   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.vendor=Oracle Corporation
[framework] 2015-12-14 17:11:45,575 - org.apache.zookeeper.ZooKeeper -43   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.home=/usr/lib/jvm/java-8-sun/jre
[framework] 2015-12-14 17:11:45,575 - org.apache.zookeeper.ZooKeeper -43   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.class.path=/home/juxinli/workspace/infogen_etl/target/classes:/home/juxinli/workspace/infogen_etl/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-client/2.7.1/hadoop-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-common/2.7.1/hadoop-common-2.7.1.jar:/home/juxinli/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/home/juxinli/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/juxinli/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/juxinli/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/juxinli/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/home/juxinli/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/juxinli/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/juxinli/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/juxinli/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/juxinli/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/juxinli/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/juxinli/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/juxinli/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/juxinli/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/juxinli/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/juxinli/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/juxinli/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-auth/2.7.1/hadoop-auth-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/home/juxinli/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/juxinli/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/juxinli/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.7.1/hadoop-hdfs-2.7.1.jar:/home/juxinli/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/juxinli/.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/home/juxinli/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/juxinli/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/juxinli/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.1/hadoop-mapreduce-client-app-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.1/hadoop-mapreduce-client-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.7.1/hadoop-yarn-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.7.1/hadoop-yarn-server-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.1/hadoop-mapreduce-client-shuffle-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.7.1/hadoop-yarn-api-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.1/hadoop-mapreduce-client-core-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.7.1/hadoop-yarn-common-2.7.1.jar:/home/juxinli/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/juxinli/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/juxinli/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/juxinli/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.1/hadoop-mapreduce-client-jobclient-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.1/hadoop-annotations-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka_2.11/0.8.2.2/kafka_2.11-0.8.2.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/juxinli/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/home/juxinli/.m2/repository/org/scala-lang/scala-library/2.11.5/scala-library-2.11.5.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka-clients/0.8.2.2/kafka-clients-0.8.2.2.jar:/home/juxinli/.m2/repository/org/xerial/snappy/snappy-java/1.1.1.7/snappy-java-1.1.1.7.jar:/home/juxinli/.m2/repository/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar:/home/juxinli/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/juxinli/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/juxinli/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/juxinli/.m2/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar
[framework] 2015-12-14 17:11:45,577 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[framework] 2015-12-14 17:11:45,577 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.io.tmpdir=/tmp
[framework] 2015-12-14 17:11:45,577 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.compiler=<NA>
[framework] 2015-12-14 17:11:45,577 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.name=Linux
[framework] 2015-12-14 17:11:45,577 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.arch=amd64
[framework] 2015-12-14 17:11:45,578 - org.apache.zookeeper.ZooKeeper -46   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.version=4.2.0-19-generic
[framework] 2015-12-14 17:11:45,578 - org.apache.zookeeper.ZooKeeper -46   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.name=juxinli
[framework] 2015-12-14 17:11:45,578 - org.apache.zookeeper.ZooKeeper -46   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.home=/home/juxinli
[framework] 2015-12-14 17:11:45,578 - org.apache.zookeeper.ZooKeeper -46   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.dir=/home/juxinli/workspace/infogen_etl
[framework] 2015-12-14 17:11:45,580 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Initiating client connection, connectString=172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181 sessionTimeout=10000 watcher=com.infogen.zookeeper.InfoGen_ZooKeeper$1@4bec1f0c
[framework] 2015-12-14 17:11:45,609 - com.infogen.zookeeper.InfoGen_ZooKeeper -77   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper成功:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:11:45,610 - com.infogen.zookeeper.InfoGen_ZooKeeper -78   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录:/brokers/ids
[framework] 2015-12-14 17:11:45,617 - org.apache.zookeeper.ClientCnxn -85   [main-SendThread(172.16.8.98:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 172.16.8.98/172.16.8.98:2181. Will not attempt to authenticate using SASL (unknown error)
[framework] 2015-12-14 17:11:45,690 - org.apache.zookeeper.ClientCnxn -158  [main-SendThread(172.16.8.98:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Socket connection established to 172.16.8.98/172.16.8.98:2181, initiating session
[framework] 2015-12-14 17:11:45,711 - org.apache.zookeeper.ClientCnxn -179  [main-SendThread(172.16.8.98:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 172.16.8.98/172.16.8.98:2181, sessionid = 0x6250c64e425606de, negotiated timeout = 10000
[framework] 2015-12-14 17:11:45,715 - com.infogen.zookeeper.InfoGen_ZooKeeper -183  [main-EventThread] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 连接事件  path:null  state:SyncConnected  type:None
[framework] 2015-12-14 17:11:45,730 - com.infogen.zookeeper.InfoGen_ZooKeeper -198  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录成功:/brokers/ids
[framework] 2015-12-14 17:11:45,730 - com.infogen.zookeeper.InfoGen_ZooKeeper -198  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/1
[framework] 2015-12-14 17:11:45,747 - com.infogen.zookeeper.InfoGen_ZooKeeper -215  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/1
[framework] 2015-12-14 17:11:45,747 - com.infogen.zookeeper.InfoGen_ZooKeeper -215  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/2
[framework] 2015-12-14 17:11:45,761 - com.infogen.zookeeper.InfoGen_ZooKeeper -229  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/2
[framework] 2015-12-14 17:11:45,762 - com.infogen.zookeeper.InfoGen_ZooKeeper -230  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/3
[framework] 2015-12-14 17:11:45,775 - com.infogen.zookeeper.InfoGen_ZooKeeper -243  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/3
[framework] 2015-12-14 17:11:45,775 - com.infogen.zookeeper.InfoGen_ZooKeeper -243  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:11:45,802 - com.infogen.zookeeper.InfoGen_ZooKeeper -270  [main] ERROR com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据错误: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /infogen_consumers/infogen_topic_tracking/1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1184)
	at com.infogen.zookeeper.InfoGen_ZooKeeper.get_data(InfoGen_ZooKeeper.java:155)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:113)
[framework] 2015-12-14 17:11:45,811 - com.infogen.zookeeper.InfoGen_ZooKeeper -279  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 创建节点:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:11:45,830 - com.infogen.zookeeper.InfoGen_ZooKeeper -298  [main] ERROR com.infogen.zookeeper.InfoGen_ZooKeeper  - 未知错误: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /infogen_consumers/infogen_topic_tracking/1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at com.infogen.zookeeper.InfoGen_ZooKeeper.create(InfoGen_ZooKeeper.java:118)
	at com.infogen.zookeeper.InfoGen_ZooKeeper.create(InfoGen_ZooKeeper.java:127)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:118)
[framework] 2015-12-14 17:11:45,831 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -299  [main] INFO  com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #使用默认 offset为:0
[framework] 2015-12-14 17:11:46,580 - com.infogen.kafka.InfoGen_Consumer -1048 [main] WARN  com.infogen.kafka.InfoGen_Consumer  - #offset不存在-从最早的offset开始获取:4494174
[framework] 2015-12-14 17:11:47,116 - org.apache.hadoop.util.NativeCodeLoader -1584 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[framework] 2015-12-14 17:11:48,071 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -2539 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:11:48,260 - com.hadoop.compression.lzo.GPLNativeCodeLoader -2728 [main] INFO  com.hadoop.compression.lzo.GPLNativeCodeLoader  - Loaded native gpl library from the embedded binaries
[framework] 2015-12-14 17:11:48,262 - com.hadoop.compression.lzo.LzoCodec -2730 [main] INFO  com.hadoop.compression.lzo.LzoCodec  - Successfully loaded & initialized native-lzo library [hadoop-lzo rev 826e7d8d3e839964dd9ed2d5f83296254b2c71d3]
[framework] 2015-12-14 17:11:48,264 - org.apache.hadoop.conf.Configuration.deprecation -2732 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[framework] 2015-12-14 17:11:48,265 - org.apache.hadoop.io.compress.CodecPool -2733 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 17:12:46,751 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -61219 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:12:47,247 - com.infogen.zookeeper.InfoGen_ZooKeeper -61715 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:12:47,266 - com.infogen.zookeeper.InfoGen_ZooKeeper -61734 [Thread-0] ERROR com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据失败: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /infogen_consumers/infogen_topic_tracking/1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)
	at com.infogen.zookeeper.InfoGen_ZooKeeper.set_data(InfoGen_ZooKeeper.java:169)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.lambda$0(Kafka_To_Hdfs.java:81)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs$$Lambda$2/1131645570.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:745)
[framework] 2015-12-14 17:19:59,134 - com.infogen.zookeeper.InfoGen_ZooKeeper -0    [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:19:59,181 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[framework] 2015-12-14 17:19:59,182 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:host.name=localhost
[framework] 2015-12-14 17:19:59,182 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.version=1.8.0_20
[framework] 2015-12-14 17:19:59,182 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.vendor=Oracle Corporation
[framework] 2015-12-14 17:19:59,182 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.home=/usr/lib/jvm/java-8-sun/jre
[framework] 2015-12-14 17:19:59,182 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.class.path=/home/juxinli/workspace/infogen_etl/target/classes:/home/juxinli/workspace/infogen_etl/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-client/2.7.1/hadoop-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-common/2.7.1/hadoop-common-2.7.1.jar:/home/juxinli/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/home/juxinli/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/juxinli/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/juxinli/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/juxinli/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/home/juxinli/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/juxinli/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/juxinli/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/juxinli/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/juxinli/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/juxinli/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/juxinli/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/juxinli/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/juxinli/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/juxinli/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/juxinli/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/juxinli/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-auth/2.7.1/hadoop-auth-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/home/juxinli/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/juxinli/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/juxinli/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.7.1/hadoop-hdfs-2.7.1.jar:/home/juxinli/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/juxinli/.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/home/juxinli/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/juxinli/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/juxinli/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.1/hadoop-mapreduce-client-app-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.1/hadoop-mapreduce-client-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.7.1/hadoop-yarn-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.7.1/hadoop-yarn-server-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.1/hadoop-mapreduce-client-shuffle-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.7.1/hadoop-yarn-api-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.1/hadoop-mapreduce-client-core-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.7.1/hadoop-yarn-common-2.7.1.jar:/home/juxinli/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/juxinli/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/juxinli/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/juxinli/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.1/hadoop-mapreduce-client-jobclient-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.1/hadoop-annotations-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka_2.11/0.8.2.2/kafka_2.11-0.8.2.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/juxinli/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/home/juxinli/.m2/repository/org/scala-lang/scala-library/2.11.5/scala-library-2.11.5.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka-clients/0.8.2.2/kafka-clients-0.8.2.2.jar:/home/juxinli/.m2/repository/org/xerial/snappy/snappy-java/1.1.1.7/snappy-java-1.1.1.7.jar:/home/juxinli/.m2/repository/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar:/home/juxinli/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/juxinli/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/juxinli/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/juxinli/.m2/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar
[framework] 2015-12-14 17:19:59,183 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[framework] 2015-12-14 17:19:59,183 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.io.tmpdir=/tmp
[framework] 2015-12-14 17:19:59,183 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.compiler=<NA>
[framework] 2015-12-14 17:19:59,183 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.name=Linux
[framework] 2015-12-14 17:19:59,183 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.arch=amd64
[framework] 2015-12-14 17:19:59,183 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.version=4.2.0-19-generic
[framework] 2015-12-14 17:19:59,183 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.name=juxinli
[framework] 2015-12-14 17:19:59,184 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.home=/home/juxinli
[framework] 2015-12-14 17:19:59,184 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.dir=/home/juxinli/workspace/infogen_etl
[framework] 2015-12-14 17:19:59,185 - org.apache.zookeeper.ZooKeeper -51   [main] INFO  org.apache.zookeeper.ZooKeeper  - Initiating client connection, connectString=172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181 sessionTimeout=10000 watcher=com.infogen.zookeeper.InfoGen_ZooKeeper$1@4bec1f0c
[framework] 2015-12-14 17:19:59,218 - com.infogen.zookeeper.InfoGen_ZooKeeper -84   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper成功:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:19:59,220 - com.infogen.zookeeper.InfoGen_ZooKeeper -86   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录:/brokers/ids
[framework] 2015-12-14 17:19:59,224 - org.apache.zookeeper.ClientCnxn -90   [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 172.16.8.97/172.16.8.97:2181. Will not attempt to authenticate using SASL (unknown error)
[framework] 2015-12-14 17:19:59,308 - org.apache.zookeeper.ClientCnxn -174  [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Socket connection established to 172.16.8.97/172.16.8.97:2181, initiating session
[framework] 2015-12-14 17:19:59,330 - org.apache.zookeeper.ClientCnxn -196  [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 172.16.8.97/172.16.8.97:2181, sessionid = 0x614f1aad4d9c8039, negotiated timeout = 10000
[framework] 2015-12-14 17:19:59,333 - com.infogen.zookeeper.InfoGen_ZooKeeper -199  [main-EventThread] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 连接事件  path:null  state:SyncConnected  type:None
[framework] 2015-12-14 17:19:59,348 - com.infogen.zookeeper.InfoGen_ZooKeeper -214  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录成功:/brokers/ids
[framework] 2015-12-14 17:19:59,348 - com.infogen.zookeeper.InfoGen_ZooKeeper -214  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/1
[framework] 2015-12-14 17:19:59,365 - com.infogen.zookeeper.InfoGen_ZooKeeper -231  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/1
[framework] 2015-12-14 17:19:59,366 - com.infogen.zookeeper.InfoGen_ZooKeeper -232  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/2
[framework] 2015-12-14 17:19:59,380 - com.infogen.zookeeper.InfoGen_ZooKeeper -246  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/2
[framework] 2015-12-14 17:19:59,381 - com.infogen.zookeeper.InfoGen_ZooKeeper -247  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/3
[framework] 2015-12-14 17:19:59,396 - com.infogen.zookeeper.InfoGen_ZooKeeper -262  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/3
[framework] 2015-12-14 17:19:59,396 - com.infogen.zookeeper.InfoGen_ZooKeeper -262  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:19:59,426 - com.infogen.zookeeper.InfoGen_ZooKeeper -292  [main] ERROR com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据错误: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /infogen_consumers/infogen_topic_tracking/1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1184)
	at com.infogen.zookeeper.InfoGen_ZooKeeper.get_data(InfoGen_ZooKeeper.java:155)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:113)
[framework] 2015-12-14 17:19:59,432 - com.infogen.zookeeper.InfoGen_ZooKeeper -298  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers
[framework] 2015-12-14 17:19:59,450 - com.infogen.zookeeper.InfoGen_ZooKeeper -316  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers
[framework] 2015-12-14 17:19:59,454 - com.infogen.zookeeper.InfoGen_ZooKeeper -320  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 创建节点:/infogen_consumers
[framework] 2015-12-14 17:19:59,472 - com.infogen.zookeeper.InfoGen_ZooKeeper -338  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 创建节点成功:/infogen_consumers
[framework] 2015-12-14 17:19:59,473 - com.infogen.zookeeper.InfoGen_ZooKeeper -339  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:19:59,486 - com.infogen.zookeeper.InfoGen_ZooKeeper -352  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:19:59,486 - com.infogen.zookeeper.InfoGen_ZooKeeper -352  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 创建节点:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:19:59,501 - com.infogen.zookeeper.InfoGen_ZooKeeper -367  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 创建节点成功:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:19:59,502 - com.infogen.zookeeper.InfoGen_ZooKeeper -368  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:19:59,515 - com.infogen.zookeeper.InfoGen_ZooKeeper -381  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:19:59,516 - com.infogen.zookeeper.InfoGen_ZooKeeper -382  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 创建节点:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:19:59,530 - com.infogen.zookeeper.InfoGen_ZooKeeper -396  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 创建节点成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:19:59,531 - com.infogen.zookeeper.InfoGen_ZooKeeper -397  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:19:59,547 - com.infogen.zookeeper.InfoGen_ZooKeeper -413  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:19:59,547 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -413  [main] INFO  com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #使用默认 offset为:0
[framework] 2015-12-14 17:20:00,264 - com.infogen.kafka.InfoGen_Consumer -1130 [main] WARN  com.infogen.kafka.InfoGen_Consumer  - #offset不存在-从最早的offset开始获取:4494174
[framework] 2015-12-14 17:20:01,658 - org.apache.hadoop.util.NativeCodeLoader -2524 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[framework] 2015-12-14 17:20:02,601 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -3467 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:20:02,880 - com.hadoop.compression.lzo.GPLNativeCodeLoader -3746 [main] INFO  com.hadoop.compression.lzo.GPLNativeCodeLoader  - Loaded native gpl library from the embedded binaries
[framework] 2015-12-14 17:20:02,885 - com.hadoop.compression.lzo.LzoCodec -3751 [main] INFO  com.hadoop.compression.lzo.LzoCodec  - Successfully loaded & initialized native-lzo library [hadoop-lzo rev 826e7d8d3e839964dd9ed2d5f83296254b2c71d3]
[framework] 2015-12-14 17:20:02,888 - org.apache.hadoop.conf.Configuration.deprecation -3754 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[framework] 2015-12-14 17:20:02,891 - org.apache.hadoop.io.compress.CodecPool -3757 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 17:20:46,808 - com.infogen.zookeeper.InfoGen_ZooKeeper -0    [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:20:46,867 - org.apache.zookeeper.ZooKeeper -59   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[framework] 2015-12-14 17:20:46,867 - org.apache.zookeeper.ZooKeeper -59   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:host.name=localhost
[framework] 2015-12-14 17:20:46,867 - org.apache.zookeeper.ZooKeeper -59   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.version=1.8.0_20
[framework] 2015-12-14 17:20:46,867 - org.apache.zookeeper.ZooKeeper -59   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.vendor=Oracle Corporation
[framework] 2015-12-14 17:20:46,868 - org.apache.zookeeper.ZooKeeper -60   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.home=/usr/lib/jvm/java-8-sun/jre
[framework] 2015-12-14 17:20:46,869 - org.apache.zookeeper.ZooKeeper -61   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.class.path=/home/juxinli/workspace/infogen_etl/target/classes:/home/juxinli/workspace/infogen_etl/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-client/2.7.1/hadoop-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-common/2.7.1/hadoop-common-2.7.1.jar:/home/juxinli/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/home/juxinli/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/juxinli/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/juxinli/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/juxinli/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/home/juxinli/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/juxinli/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/juxinli/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/juxinli/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/juxinli/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/juxinli/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/juxinli/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/juxinli/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/juxinli/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/juxinli/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/juxinli/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/juxinli/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-auth/2.7.1/hadoop-auth-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/home/juxinli/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/juxinli/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/juxinli/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.7.1/hadoop-hdfs-2.7.1.jar:/home/juxinli/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/juxinli/.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/home/juxinli/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/juxinli/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/juxinli/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.1/hadoop-mapreduce-client-app-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.1/hadoop-mapreduce-client-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.7.1/hadoop-yarn-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.7.1/hadoop-yarn-server-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.1/hadoop-mapreduce-client-shuffle-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.7.1/hadoop-yarn-api-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.1/hadoop-mapreduce-client-core-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.7.1/hadoop-yarn-common-2.7.1.jar:/home/juxinli/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/juxinli/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/juxinli/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/juxinli/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.1/hadoop-mapreduce-client-jobclient-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.1/hadoop-annotations-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka_2.11/0.8.2.2/kafka_2.11-0.8.2.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/juxinli/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/home/juxinli/.m2/repository/org/scala-lang/scala-library/2.11.5/scala-library-2.11.5.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka-clients/0.8.2.2/kafka-clients-0.8.2.2.jar:/home/juxinli/.m2/repository/org/xerial/snappy/snappy-java/1.1.1.7/snappy-java-1.1.1.7.jar:/home/juxinli/.m2/repository/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar:/home/juxinli/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/juxinli/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/juxinli/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/juxinli/.m2/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar
[framework] 2015-12-14 17:20:46,872 - org.apache.zookeeper.ZooKeeper -64   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[framework] 2015-12-14 17:20:46,872 - org.apache.zookeeper.ZooKeeper -64   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.io.tmpdir=/tmp
[framework] 2015-12-14 17:20:46,872 - org.apache.zookeeper.ZooKeeper -64   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.compiler=<NA>
[framework] 2015-12-14 17:20:46,872 - org.apache.zookeeper.ZooKeeper -64   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.name=Linux
[framework] 2015-12-14 17:20:46,873 - org.apache.zookeeper.ZooKeeper -65   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.arch=amd64
[framework] 2015-12-14 17:20:46,873 - org.apache.zookeeper.ZooKeeper -65   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.version=4.2.0-19-generic
[framework] 2015-12-14 17:20:46,873 - org.apache.zookeeper.ZooKeeper -65   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.name=juxinli
[framework] 2015-12-14 17:20:46,873 - org.apache.zookeeper.ZooKeeper -65   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.home=/home/juxinli
[framework] 2015-12-14 17:20:46,873 - org.apache.zookeeper.ZooKeeper -65   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.dir=/home/juxinli/workspace/infogen_etl
[framework] 2015-12-14 17:20:46,875 - org.apache.zookeeper.ZooKeeper -67   [main] INFO  org.apache.zookeeper.ZooKeeper  - Initiating client connection, connectString=172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181 sessionTimeout=10000 watcher=com.infogen.zookeeper.InfoGen_ZooKeeper$1@4bec1f0c
[framework] 2015-12-14 17:20:46,927 - com.infogen.zookeeper.InfoGen_ZooKeeper -119  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper成功:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:20:46,929 - com.infogen.zookeeper.InfoGen_ZooKeeper -121  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录:/brokers/ids
[framework] 2015-12-14 17:20:46,938 - org.apache.zookeeper.ClientCnxn -130  [main-SendThread(172.16.8.98:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 172.16.8.98/172.16.8.98:2181. Will not attempt to authenticate using SASL (unknown error)
[framework] 2015-12-14 17:20:47,031 - org.apache.zookeeper.ClientCnxn -223  [main-SendThread(172.16.8.98:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Socket connection established to 172.16.8.98/172.16.8.98:2181, initiating session
[framework] 2015-12-14 17:20:47,063 - org.apache.zookeeper.ClientCnxn -255  [main-SendThread(172.16.8.98:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 172.16.8.98/172.16.8.98:2181, sessionid = 0x6250c64e425606df, negotiated timeout = 10000
[framework] 2015-12-14 17:20:47,066 - com.infogen.zookeeper.InfoGen_ZooKeeper -258  [main-EventThread] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 连接事件  path:null  state:SyncConnected  type:None
[framework] 2015-12-14 17:20:47,090 - com.infogen.zookeeper.InfoGen_ZooKeeper -282  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录成功:/brokers/ids
[framework] 2015-12-14 17:20:47,090 - com.infogen.zookeeper.InfoGen_ZooKeeper -282  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/1
[framework] 2015-12-14 17:20:47,110 - com.infogen.zookeeper.InfoGen_ZooKeeper -302  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/1
[framework] 2015-12-14 17:20:47,111 - com.infogen.zookeeper.InfoGen_ZooKeeper -303  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/2
[framework] 2015-12-14 17:20:47,128 - com.infogen.zookeeper.InfoGen_ZooKeeper -320  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/2
[framework] 2015-12-14 17:20:47,129 - com.infogen.zookeeper.InfoGen_ZooKeeper -321  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/3
[framework] 2015-12-14 17:20:47,147 - com.infogen.zookeeper.InfoGen_ZooKeeper -339  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/3
[framework] 2015-12-14 17:20:47,147 - com.infogen.zookeeper.InfoGen_ZooKeeper -339  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers
[framework] 2015-12-14 17:20:47,174 - com.infogen.zookeeper.InfoGen_ZooKeeper -366  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers
[framework] 2015-12-14 17:20:47,174 - com.infogen.zookeeper.InfoGen_ZooKeeper -366  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:20:47,197 - com.infogen.zookeeper.InfoGen_ZooKeeper -389  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:20:47,198 - com.infogen.zookeeper.InfoGen_ZooKeeper -390  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:20:47,219 - com.infogen.zookeeper.InfoGen_ZooKeeper -411  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:20:47,219 - com.infogen.zookeeper.InfoGen_ZooKeeper -411  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:20:47,238 - com.infogen.zookeeper.InfoGen_ZooKeeper -430  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:20:47,238 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -430  [main] INFO  com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #使用zookeeper 中 offset为:0
[framework] 2015-12-14 17:20:47,938 - com.infogen.kafka.InfoGen_Consumer -1130 [main] WARN  com.infogen.kafka.InfoGen_Consumer  - #offset不存在-从最早的offset开始获取:4494174
[framework] 2015-12-14 17:20:48,390 - org.apache.hadoop.util.NativeCodeLoader -1582 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[framework] 2015-12-14 17:20:49,484 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -2676 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:20:49,543 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -2735 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to create file [/infogen/output/infogen_topic_tracking/1/4494174-] for [DFSClient_NONMAPREDUCE_-1088967366_1] for client [192.168.100.203], because this file is already being created by [DFSClient_NONMAPREDUCE_759773558_1] on [192.168.100.203]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3035)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2737)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:49)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:134)
[framework] 2015-12-14 17:20:50,700 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -3892 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:20:50,722 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -3914 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to create file [/infogen/output/infogen_topic_tracking/1/4494174-] for [DFSClient_NONMAPREDUCE_-1088967366_1] for client [192.168.100.203], because this file is already being created by [DFSClient_NONMAPREDUCE_759773558_1] on [192.168.100.203]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3035)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2737)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:49)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:134)
[framework] 2015-12-14 17:20:51,809 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -5001 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:20:51,825 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -5017 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to create file [/infogen/output/infogen_topic_tracking/1/4494174-] for [DFSClient_NONMAPREDUCE_-1088967366_1] for client [192.168.100.203], because this file is already being created by [DFSClient_NONMAPREDUCE_759773558_1] on [192.168.100.203]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3035)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2737)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:49)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:134)
[framework] 2015-12-14 17:20:52,960 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -6152 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:20:52,983 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -6175 [main] ERROR com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #写入hdfs失败:
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to create file [/infogen/output/infogen_topic_tracking/1/4494174-] for [DFSClient_NONMAPREDUCE_-1088967366_1] for client [192.168.100.203], because this file is already being created by [DFSClient_NONMAPREDUCE_759773558_1] on [192.168.100.203]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3035)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2737)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1476)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy11.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1623)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
	at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
	at com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream.<init>(InfoGen_Hdfs_LZOOutputStream.java:62)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.handle(Kafka_To_Hdfs.java:49)
	at com.infogen.kafka.InfoGen_Consumer.run(InfoGen_Consumer.java:147)
	at com.infogen.kafka.InfoGen_Consumer.start(InfoGen_Consumer.java:57)
	at com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs.main(Kafka_To_Hdfs.java:134)
[framework] 2015-12-14 17:22:06,024 - com.infogen.zookeeper.InfoGen_ZooKeeper -0    [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:22:06,069 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[framework] 2015-12-14 17:22:06,069 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:host.name=localhost
[framework] 2015-12-14 17:22:06,069 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.version=1.8.0_20
[framework] 2015-12-14 17:22:06,069 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.vendor=Oracle Corporation
[framework] 2015-12-14 17:22:06,069 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.home=/usr/lib/jvm/java-8-sun/jre
[framework] 2015-12-14 17:22:06,069 - org.apache.zookeeper.ZooKeeper -45   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.class.path=/home/juxinli/workspace/infogen_etl/target/classes:/home/juxinli/workspace/infogen_etl/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-client/2.7.1/hadoop-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-common/2.7.1/hadoop-common-2.7.1.jar:/home/juxinli/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/home/juxinli/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/juxinli/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/juxinli/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/juxinli/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/home/juxinli/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/juxinli/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/juxinli/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/juxinli/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/juxinli/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/juxinli/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/juxinli/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/juxinli/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/juxinli/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/juxinli/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/juxinli/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/juxinli/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-auth/2.7.1/hadoop-auth-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/home/juxinli/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/juxinli/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/juxinli/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.7.1/hadoop-hdfs-2.7.1.jar:/home/juxinli/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/juxinli/.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/home/juxinli/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/juxinli/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/juxinli/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.1/hadoop-mapreduce-client-app-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.1/hadoop-mapreduce-client-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.7.1/hadoop-yarn-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.7.1/hadoop-yarn-server-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.1/hadoop-mapreduce-client-shuffle-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.7.1/hadoop-yarn-api-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.1/hadoop-mapreduce-client-core-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.7.1/hadoop-yarn-common-2.7.1.jar:/home/juxinli/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/juxinli/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/juxinli/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/juxinli/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.1/hadoop-mapreduce-client-jobclient-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.1/hadoop-annotations-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka_2.11/0.8.2.2/kafka_2.11-0.8.2.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/juxinli/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/home/juxinli/.m2/repository/org/scala-lang/scala-library/2.11.5/scala-library-2.11.5.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka-clients/0.8.2.2/kafka-clients-0.8.2.2.jar:/home/juxinli/.m2/repository/org/xerial/snappy/snappy-java/1.1.1.7/snappy-java-1.1.1.7.jar:/home/juxinli/.m2/repository/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar:/home/juxinli/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/juxinli/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/juxinli/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/juxinli/.m2/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar
[framework] 2015-12-14 17:22:06,070 - org.apache.zookeeper.ZooKeeper -46   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[framework] 2015-12-14 17:22:06,071 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.io.tmpdir=/tmp
[framework] 2015-12-14 17:22:06,071 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.compiler=<NA>
[framework] 2015-12-14 17:22:06,071 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.name=Linux
[framework] 2015-12-14 17:22:06,071 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.arch=amd64
[framework] 2015-12-14 17:22:06,071 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.version=4.2.0-19-generic
[framework] 2015-12-14 17:22:06,071 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.name=juxinli
[framework] 2015-12-14 17:22:06,071 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.home=/home/juxinli
[framework] 2015-12-14 17:22:06,072 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.dir=/home/juxinli/workspace/infogen_etl
[framework] 2015-12-14 17:22:06,073 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Initiating client connection, connectString=172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181 sessionTimeout=10000 watcher=com.infogen.zookeeper.InfoGen_ZooKeeper$1@4bec1f0c
[framework] 2015-12-14 17:22:06,108 - com.infogen.zookeeper.InfoGen_ZooKeeper -84   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper成功:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 17:22:06,108 - com.infogen.zookeeper.InfoGen_ZooKeeper -84   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录:/brokers/ids
[framework] 2015-12-14 17:22:06,117 - org.apache.zookeeper.ClientCnxn -93   [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 172.16.8.97/172.16.8.97:2181. Will not attempt to authenticate using SASL (unknown error)
[framework] 2015-12-14 17:22:06,189 - org.apache.zookeeper.ClientCnxn -165  [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Socket connection established to 172.16.8.97/172.16.8.97:2181, initiating session
[framework] 2015-12-14 17:22:06,210 - org.apache.zookeeper.ClientCnxn -186  [main-SendThread(172.16.8.97:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 172.16.8.97/172.16.8.97:2181, sessionid = 0x614f1aad4d9c803e, negotiated timeout = 10000
[framework] 2015-12-14 17:22:06,212 - com.infogen.zookeeper.InfoGen_ZooKeeper -188  [main-EventThread] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 连接事件  path:null  state:SyncConnected  type:None
[framework] 2015-12-14 17:22:06,226 - com.infogen.zookeeper.InfoGen_ZooKeeper -202  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录成功:/brokers/ids
[framework] 2015-12-14 17:22:06,226 - com.infogen.zookeeper.InfoGen_ZooKeeper -202  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/1
[framework] 2015-12-14 17:22:06,242 - com.infogen.zookeeper.InfoGen_ZooKeeper -218  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/1
[framework] 2015-12-14 17:22:06,242 - com.infogen.zookeeper.InfoGen_ZooKeeper -218  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/2
[framework] 2015-12-14 17:22:06,256 - com.infogen.zookeeper.InfoGen_ZooKeeper -232  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/2
[framework] 2015-12-14 17:22:06,256 - com.infogen.zookeeper.InfoGen_ZooKeeper -232  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/3
[framework] 2015-12-14 17:22:06,269 - com.infogen.zookeeper.InfoGen_ZooKeeper -245  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/3
[framework] 2015-12-14 17:22:06,270 - com.infogen.zookeeper.InfoGen_ZooKeeper -246  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers
[framework] 2015-12-14 17:22:06,285 - com.infogen.zookeeper.InfoGen_ZooKeeper -261  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers
[framework] 2015-12-14 17:22:06,285 - com.infogen.zookeeper.InfoGen_ZooKeeper -261  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:22:06,298 - com.infogen.zookeeper.InfoGen_ZooKeeper -274  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 17:22:06,299 - com.infogen.zookeeper.InfoGen_ZooKeeper -275  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:22:06,312 - com.infogen.zookeeper.InfoGen_ZooKeeper -288  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:22:06,312 - com.infogen.zookeeper.InfoGen_ZooKeeper -288  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:22:06,327 - com.infogen.zookeeper.InfoGen_ZooKeeper -303  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:22:06,328 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -304  [main] INFO  com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #使用zookeeper 中 offset为:0
[framework] 2015-12-14 17:22:07,069 - com.infogen.kafka.InfoGen_Consumer -1045 [main] WARN  com.infogen.kafka.InfoGen_Consumer  - #offset不存在-从最早的offset开始获取:4494174
[framework] 2015-12-14 17:22:07,548 - org.apache.hadoop.util.NativeCodeLoader -1524 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[framework] 2015-12-14 17:22:08,989 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -2965 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:22:09,192 - com.hadoop.compression.lzo.GPLNativeCodeLoader -3168 [main] INFO  com.hadoop.compression.lzo.GPLNativeCodeLoader  - Loaded native gpl library from the embedded binaries
[framework] 2015-12-14 17:22:09,196 - com.hadoop.compression.lzo.LzoCodec -3172 [main] INFO  com.hadoop.compression.lzo.LzoCodec  - Successfully loaded & initialized native-lzo library [hadoop-lzo rev 826e7d8d3e839964dd9ed2d5f83296254b2c71d3]
[framework] 2015-12-14 17:22:09,199 - org.apache.hadoop.conf.Configuration.deprecation -3175 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[framework] 2015-12-14 17:22:09,202 - org.apache.hadoop.io.compress.CodecPool -3178 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 17:23:07,191 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -61167 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4494174-
[framework] 2015-12-14 17:23:07,597 - com.infogen.zookeeper.InfoGen_ZooKeeper -61573 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:23:07,615 - com.infogen.zookeeper.InfoGen_ZooKeeper -61591 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:23:40,856 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -94832 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675184-
[framework] 2015-12-14 17:24:40,731 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -154707 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675184-
[framework] 2015-12-14 17:24:41,248 - com.infogen.zookeeper.InfoGen_ZooKeeper -155224 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:24:41,264 - com.infogen.zookeeper.InfoGen_ZooKeeper -155240 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:24:45,425 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -159401 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675234-
[framework] 2015-12-14 17:25:45,328 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -219304 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675234-
[framework] 2015-12-14 17:25:45,839 - com.infogen.zookeeper.InfoGen_ZooKeeper -219815 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:25:45,855 - com.infogen.zookeeper.InfoGen_ZooKeeper -219831 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:25:47,767 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -221743 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675270-
[framework] 2015-12-14 17:26:47,684 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -281660 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675270-
[framework] 2015-12-14 17:26:48,324 - com.infogen.zookeeper.InfoGen_ZooKeeper -282300 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:26:48,340 - com.infogen.zookeeper.InfoGen_ZooKeeper -282316 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:26:48,927 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -282903 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675330-
[framework] 2015-12-14 17:27:48,857 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -342833 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675330-
[framework] 2015-12-14 17:27:49,487 - com.infogen.zookeeper.InfoGen_ZooKeeper -343463 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:27:49,502 - com.infogen.zookeeper.InfoGen_ZooKeeper -343478 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:27:50,391 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -344367 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675389-
[framework] 2015-12-14 17:28:50,298 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -404274 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675389-
[framework] 2015-12-14 17:28:51,057 - com.infogen.zookeeper.InfoGen_ZooKeeper -405033 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:28:51,075 - com.infogen.zookeeper.InfoGen_ZooKeeper -405051 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:28:51,228 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -405204 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675410-
[framework] 2015-12-14 17:29:51,164 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -465140 [Thread-0] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #关闭流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675410-
[framework] 2015-12-14 17:29:51,620 - com.infogen.zookeeper.InfoGen_ZooKeeper -465596 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:29:51,634 - com.infogen.zookeeper.InfoGen_ZooKeeper -465610 [Thread-0] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 写入节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 17:29:52,540 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -466516 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/1/4675436-
[framework] 2015-12-14 17:52:42,616 - com.infogen.kafka.InfoGen_Consumer -0    [main] WARN  com.infogen.kafka.InfoGen_Consumer  - #offset不存在-从最早的offset开始获取:4360015
[framework] 2015-12-14 17:54:50,892 - com.infogen.kafka.InfoGen_Consumer -0    [main] WARN  com.infogen.kafka.InfoGen_Consumer  - #offset不存在-从最早的offset开始获取:4360015
[framework] 2015-12-14 19:29:18,827 - com.infogen.zookeeper.InfoGen_ZooKeeper -0    [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 19:29:18,874 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[framework] 2015-12-14 19:29:18,874 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:host.name=localhost
[framework] 2015-12-14 19:29:18,874 - org.apache.zookeeper.ZooKeeper -47   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.version=1.8.0_20
[framework] 2015-12-14 19:29:18,875 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.vendor=Oracle Corporation
[framework] 2015-12-14 19:29:18,875 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.home=/usr/lib/jvm/java-8-sun/jre
[framework] 2015-12-14 19:29:18,875 - org.apache.zookeeper.ZooKeeper -48   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.class.path=/home/juxinli/workspace/infogen_etl/target/classes:/home/juxinli/workspace/infogen_etl/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-client/2.7.1/hadoop-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-common/2.7.1/hadoop-common-2.7.1.jar:/home/juxinli/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/home/juxinli/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/juxinli/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/juxinli/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/juxinli/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/home/juxinli/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/juxinli/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/juxinli/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/juxinli/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/juxinli/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/juxinli/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/juxinli/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/juxinli/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/juxinli/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/juxinli/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/home/juxinli/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/juxinli/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/juxinli/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/juxinli/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-auth/2.7.1/hadoop-auth-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/juxinli/.m2/repository/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/home/juxinli/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/juxinli/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/juxinli/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/juxinli/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.7.1/hadoop-hdfs-2.7.1.jar:/home/juxinli/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/juxinli/.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/home/juxinli/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/juxinli/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/home/juxinli/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.7.1/hadoop-mapreduce-client-app-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.7.1/hadoop-mapreduce-client-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.7.1/hadoop-yarn-client-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.7.1/hadoop-yarn-server-common-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.7.1/hadoop-mapreduce-client-shuffle-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.7.1/hadoop-yarn-api-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.1/hadoop-mapreduce-client-core-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.7.1/hadoop-yarn-common-2.7.1.jar:/home/juxinli/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/juxinli/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/juxinli/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/juxinli/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/juxinli/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/juxinli/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.7.1/hadoop-mapreduce-client-jobclient-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.1/hadoop-annotations-2.7.1.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka_2.11/0.8.2.2/kafka_2.11-0.8.2.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/juxinli/.m2/repository/net/sf/jopt-simple/jopt-simple/3.2/jopt-simple-3.2.jar:/home/juxinli/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar:/home/juxinli/.m2/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/home/juxinli/.m2/repository/org/scala-lang/scala-library/2.11.5/scala-library-2.11.5.jar:/home/juxinli/.m2/repository/org/apache/kafka/kafka-clients/0.8.2.2/kafka-clients-0.8.2.2.jar:/home/juxinli/.m2/repository/org/xerial/snappy/snappy-java/1.1.1.7/snappy-java-1.1.1.7.jar:/home/juxinli/.m2/repository/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar:/home/juxinli/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/juxinli/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/juxinli/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar:/home/juxinli/.m2/repository/io/netty/netty/3.7.0.Final/netty-3.7.0.Final.jar
[framework] 2015-12-14 19:29:18,876 - org.apache.zookeeper.ZooKeeper -49   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.io.tmpdir=/tmp
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:java.compiler=<NA>
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.name=Linux
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.arch=amd64
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:os.version=4.2.0-19-generic
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.name=juxinli
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.home=/home/juxinli
[framework] 2015-12-14 19:29:18,877 - org.apache.zookeeper.ZooKeeper -50   [main] INFO  org.apache.zookeeper.ZooKeeper  - Client environment:user.dir=/home/juxinli/workspace/infogen_etl
[framework] 2015-12-14 19:29:18,879 - org.apache.zookeeper.ZooKeeper -52   [main] INFO  org.apache.zookeeper.ZooKeeper  - Initiating client connection, connectString=172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181 sessionTimeout=10000 watcher=com.infogen.zookeeper.InfoGen_ZooKeeper$1@4bec1f0c
[framework] 2015-12-14 19:29:18,915 - com.infogen.zookeeper.InfoGen_ZooKeeper -88   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 启动zookeeper成功:172.16.8.97:2181,172.16.8.98:2181,172.16.8.99:2181
[framework] 2015-12-14 19:29:18,915 - com.infogen.zookeeper.InfoGen_ZooKeeper -88   [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录:/brokers/ids
[framework] 2015-12-14 19:29:18,925 - org.apache.zookeeper.ClientCnxn -98   [main-SendThread(172.16.8.99:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 172.16.8.99/172.16.8.99:2181. Will not attempt to authenticate using SASL (unknown error)
[framework] 2015-12-14 19:29:19,007 - org.apache.zookeeper.ClientCnxn -180  [main-SendThread(172.16.8.99:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Socket connection established to 172.16.8.99/172.16.8.99:2181, initiating session
[framework] 2015-12-14 19:29:19,033 - org.apache.zookeeper.ClientCnxn -206  [main-SendThread(172.16.8.99:2181)] INFO  org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 172.16.8.99/172.16.8.99:2181, sessionid = 0x634f1aacb9811f53, negotiated timeout = 10000
[framework] 2015-12-14 19:29:19,037 - com.infogen.zookeeper.InfoGen_ZooKeeper -210  [main-EventThread] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 连接事件  path:null  state:SyncConnected  type:None
[framework] 2015-12-14 19:29:19,051 - com.infogen.zookeeper.InfoGen_ZooKeeper -224  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取子节点目录成功:/brokers/ids
[framework] 2015-12-14 19:29:19,051 - com.infogen.zookeeper.InfoGen_ZooKeeper -224  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/1
[framework] 2015-12-14 19:29:19,069 - com.infogen.zookeeper.InfoGen_ZooKeeper -242  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/1
[framework] 2015-12-14 19:29:19,070 - com.infogen.zookeeper.InfoGen_ZooKeeper -243  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/2
[framework] 2015-12-14 19:29:19,083 - com.infogen.zookeeper.InfoGen_ZooKeeper -256  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/2
[framework] 2015-12-14 19:29:19,085 - com.infogen.zookeeper.InfoGen_ZooKeeper -258  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/brokers/ids/3
[framework] 2015-12-14 19:29:19,099 - com.infogen.zookeeper.InfoGen_ZooKeeper -272  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/brokers/ids/3
[framework] 2015-12-14 19:29:19,099 - com.infogen.zookeeper.InfoGen_ZooKeeper -272  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers
[framework] 2015-12-14 19:29:19,114 - com.infogen.zookeeper.InfoGen_ZooKeeper -287  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers
[framework] 2015-12-14 19:29:19,115 - com.infogen.zookeeper.InfoGen_ZooKeeper -288  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 19:29:19,128 - com.infogen.zookeeper.InfoGen_ZooKeeper -301  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking
[framework] 2015-12-14 19:29:19,129 - com.infogen.zookeeper.InfoGen_ZooKeeper -302  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 19:29:19,142 - com.infogen.zookeeper.InfoGen_ZooKeeper -315  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 判断节点是否存在成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 19:29:19,143 - com.infogen.zookeeper.InfoGen_ZooKeeper -316  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 19:29:19,156 - com.infogen.zookeeper.InfoGen_ZooKeeper -329  [main] INFO  com.infogen.zookeeper.InfoGen_ZooKeeper  - 获取节点数据成功:/infogen_consumers/infogen_topic_tracking/1
[framework] 2015-12-14 19:29:19,156 - com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs -329  [main] INFO  com.infogen.etl.kafka_to_hdfs.Kafka_To_Hdfs  - #使用zookeeper 中 offset为:0
[framework] 2015-12-14 19:29:20,522 - org.apache.hadoop.util.NativeCodeLoader -1695 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[framework] 2015-12-14 19:29:21,626 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -2799 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/29/1.4675435-
[framework] 2015-12-14 19:29:21,834 - com.hadoop.compression.lzo.GPLNativeCodeLoader -3007 [main] INFO  com.hadoop.compression.lzo.GPLNativeCodeLoader  - Loaded native gpl library from the embedded binaries
[framework] 2015-12-14 19:29:21,838 - com.hadoop.compression.lzo.LzoCodec -3011 [main] INFO  com.hadoop.compression.lzo.LzoCodec  - Successfully loaded & initialized native-lzo library [hadoop-lzo rev 826e7d8d3e839964dd9ed2d5f83296254b2c71d3]
[framework] 2015-12-14 19:29:21,840 - org.apache.hadoop.conf.Configuration.deprecation -3013 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[framework] 2015-12-14 19:29:21,843 - org.apache.hadoop.io.compress.CodecPool -3016 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:21,940 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -3113 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/29/1.4675436-
[framework] 2015-12-14 19:29:22,020 - org.apache.hadoop.io.compress.CodecPool -3193 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:22,109 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -3282 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/29/1.4675437-
[framework] 2015-12-14 19:29:22,198 - org.apache.hadoop.io.compress.CodecPool -3371 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:22,298 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -3471 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/29/1.4675438-
[framework] 2015-12-14 19:29:22,411 - org.apache.hadoop.io.compress.CodecPool -3584 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:22,516 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -3689 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675439-
[framework] 2015-12-14 19:29:22,629 - org.apache.hadoop.io.compress.CodecPool -3802 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:22,710 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -3883 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675440-
[framework] 2015-12-14 19:29:22,802 - org.apache.hadoop.io.compress.CodecPool -3975 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:22,904 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -4077 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675441-
[framework] 2015-12-14 19:29:22,985 - org.apache.hadoop.io.compress.CodecPool -4158 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:23,087 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -4260 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675442-
[framework] 2015-12-14 19:29:23,177 - org.apache.hadoop.io.compress.CodecPool -4350 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:23,247 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -4420 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675443-
[framework] 2015-12-14 19:29:23,335 - org.apache.hadoop.io.compress.CodecPool -4508 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:23,401 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -4574 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675444-
[framework] 2015-12-14 19:29:23,486 - org.apache.hadoop.io.compress.CodecPool -4659 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:23,563 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -4736 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675445-
[framework] 2015-12-14 19:29:23,643 - org.apache.hadoop.io.compress.CodecPool -4816 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:23,702 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -4875 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675446-
[framework] 2015-12-14 19:29:23,897 - org.apache.hadoop.io.compress.CodecPool -5070 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:24,014 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -5187 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675447-
[framework] 2015-12-14 19:29:24,186 - org.apache.hadoop.io.compress.CodecPool -5359 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:24,250 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -5423 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675448-
[framework] 2015-12-14 19:29:24,377 - org.apache.hadoop.io.compress.CodecPool -5550 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:24,440 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -5613 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675449-
[framework] 2015-12-14 19:29:24,603 - org.apache.hadoop.io.compress.CodecPool -5776 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:24,664 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -5837 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675450-
[framework] 2015-12-14 19:29:24,760 - org.apache.hadoop.io.compress.CodecPool -5933 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:24,832 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -6005 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675451-
[framework] 2015-12-14 19:29:24,928 - org.apache.hadoop.io.compress.CodecPool -6101 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:24,992 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -6165 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675452-
[framework] 2015-12-14 19:29:25,095 - org.apache.hadoop.io.compress.CodecPool -6268 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:25,168 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -6341 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675453-
[framework] 2015-12-14 19:29:25,269 - org.apache.hadoop.io.compress.CodecPool -6442 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:25,360 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -6533 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675454-
[framework] 2015-12-14 19:29:25,460 - org.apache.hadoop.io.compress.CodecPool -6633 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:25,518 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -6691 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675455-
[framework] 2015-12-14 19:29:25,699 - org.apache.hadoop.io.compress.CodecPool -6872 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:25,771 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -6944 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675456-
[framework] 2015-12-14 19:29:25,860 - org.apache.hadoop.io.compress.CodecPool -7033 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:25,935 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -7108 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675457-
[framework] 2015-12-14 19:29:26,019 - org.apache.hadoop.io.compress.CodecPool -7192 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:26,082 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -7255 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675458-
[framework] 2015-12-14 19:29:26,169 - org.apache.hadoop.io.compress.CodecPool -7342 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:26,236 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -7409 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675459-
[framework] 2015-12-14 19:29:26,328 - org.apache.hadoop.io.compress.CodecPool -7501 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:26,389 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -7562 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675460-
[framework] 2015-12-14 19:29:26,492 - org.apache.hadoop.io.compress.CodecPool -7665 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:26,585 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -7758 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675461-
[framework] 2015-12-14 19:29:26,678 - org.apache.hadoop.io.compress.CodecPool -7851 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:26,739 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -7912 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675462-
[framework] 2015-12-14 19:29:26,828 - org.apache.hadoop.io.compress.CodecPool -8001 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:26,894 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -8067 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675463-
[framework] 2015-12-14 19:29:26,978 - org.apache.hadoop.io.compress.CodecPool -8151 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:27,042 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -8215 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675464-
[framework] 2015-12-14 19:29:27,135 - org.apache.hadoop.io.compress.CodecPool -8308 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:27,200 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -8373 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675465-
[framework] 2015-12-14 19:29:27,279 - org.apache.hadoop.io.compress.CodecPool -8452 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:27,337 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -8510 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675466-
[framework] 2015-12-14 19:29:27,419 - org.apache.hadoop.io.compress.CodecPool -8592 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:27,478 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -8651 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675467-
[framework] 2015-12-14 19:29:27,569 - org.apache.hadoop.io.compress.CodecPool -8742 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:27,635 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -8808 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675468-
[framework] 2015-12-14 19:29:27,727 - org.apache.hadoop.io.compress.CodecPool -8900 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:27,785 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -8958 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675469-
[framework] 2015-12-14 19:29:27,860 - org.apache.hadoop.io.compress.CodecPool -9033 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:27,925 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -9098 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675470-
[framework] 2015-12-14 19:29:28,010 - org.apache.hadoop.io.compress.CodecPool -9183 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:28,077 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -9250 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675471-
[framework] 2015-12-14 19:29:28,177 - org.apache.hadoop.io.compress.CodecPool -9350 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:28,247 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -9420 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675472-
[framework] 2015-12-14 19:29:28,327 - org.apache.hadoop.io.compress.CodecPool -9500 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:28,395 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -9568 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675473-
[framework] 2015-12-14 19:29:28,597 - org.apache.hadoop.io.compress.CodecPool -9770 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:28,661 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -9834 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675474-
[framework] 2015-12-14 19:29:28,794 - org.apache.hadoop.io.compress.CodecPool -9967 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:28,863 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -10036 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675475-
[framework] 2015-12-14 19:29:29,080 - org.apache.hadoop.io.compress.CodecPool -10253 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:29,156 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -10329 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675476-
[framework] 2015-12-14 19:29:29,257 - org.apache.hadoop.io.compress.CodecPool -10430 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:29,316 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -10489 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675477-
[framework] 2015-12-14 19:29:29,618 - org.apache.hadoop.io.compress.CodecPool -10791 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:29,683 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -10856 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675478-
[framework] 2015-12-14 19:29:29,876 - org.apache.hadoop.io.compress.CodecPool -11049 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:29,940 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -11113 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675479-
[framework] 2015-12-14 19:29:30,018 - org.apache.hadoop.io.compress.CodecPool -11191 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:30,085 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -11258 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675480-
[framework] 2015-12-14 19:29:30,168 - org.apache.hadoop.io.compress.CodecPool -11341 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:30,229 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -11402 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675481-
[framework] 2015-12-14 19:29:30,360 - org.apache.hadoop.io.compress.CodecPool -11533 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:30,425 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -11598 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675482-
[framework] 2015-12-14 19:29:30,501 - org.apache.hadoop.io.compress.CodecPool -11674 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:30,565 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -11738 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675483-
[framework] 2015-12-14 19:29:30,651 - org.apache.hadoop.io.compress.CodecPool -11824 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:30,721 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -11894 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675484-
[framework] 2015-12-14 19:29:30,810 - org.apache.hadoop.io.compress.CodecPool -11983 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:30,872 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -12045 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675485-
[framework] 2015-12-14 19:29:30,959 - org.apache.hadoop.io.compress.CodecPool -12132 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:31,018 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -12191 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675486-
[framework] 2015-12-14 19:29:31,109 - org.apache.hadoop.io.compress.CodecPool -12282 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:31,170 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -12343 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675487-
[framework] 2015-12-14 19:29:31,243 - org.apache.hadoop.io.compress.CodecPool -12416 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:31,331 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -12504 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675488-
[framework] 2015-12-14 19:29:31,426 - org.apache.hadoop.io.compress.CodecPool -12599 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:31,485 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -12658 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675489-
[framework] 2015-12-14 19:29:31,576 - org.apache.hadoop.io.compress.CodecPool -12749 [main] INFO  org.apache.hadoop.io.compress.CodecPool  - Got brand-new compressor [.lzo]
[framework] 2015-12-14 19:29:31,634 - com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream -12807 [main] INFO  com.infogen.hdfs.InfoGen_Hdfs_LZOOutputStream  - #创建流-写入LZO文件并使用索引:hdfs://spark101:8020/infogen/output/infogen_topic_tracking/2015-12-14/17/30/1.4675490-
